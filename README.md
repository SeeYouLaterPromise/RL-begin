# Begin to learn Reinforcement Learning for Super Mario

Attention: when executing `pip install -r requirements.txt`, you should quit the VPN.

## Preliminary

### Environment Configuration

Thanks for "https://www.bilibili.com/video/BV1CERYY3EjA?spm_id_from=333.788.videopod.sections&vd_source=be632c59a4ce49cc99bcd97058a50691&p=11"'s help:

```bash
conda create -n rl_mario python=3.8
conda activate rl_mario

pip install setuptools==65.5.0
pip install wheel==0.38.4
python.exe -m pip install pip==20.2.4

pip install -r requirement.txt
```

Pay attention: `gym==0.21.0` (must), if you follow the video, get the updatest version of `gym`, you will meet error like:

```
Traceback (most recent call last):
  File ".\human_op.py", line 49, in <module>
    state, _, done, _ = env.step(current_action)
  File "D:\Anaconda\envs\rl\lib\site-packages\nes_py\wrappers\joypad_space.py", line 74, in step
    return self.env.step(self._action_map[action])
  File "D:\Anaconda\envs\rl\lib\site-packages\gym\wrappers\time_limit.py", line 50, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
ValueError: not enough values to unpack (expected 5, got 4)
```

### gym

```
reward: 0.0,
done: False,
info: {
  'coins': 0,
  'flag_get': False,
  'life': 2,
  'score': 0,
  'stage': 1,
  'status': 'small',
  'time': 341, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}
```

### ğŸ® game handle buttons controlling

åœ¨é©¬é‡Œå¥¥ NES æ¸¸æˆï¼ˆä¹Ÿå°±æ˜¯ä»»å¤©å ‚çº¢ç™½æœºä¸Šçš„ç»å…¸ã€ŠSuper Mario Bros.ã€‹ï¼‰ä¸­ï¼Œ**`A` å’Œ `B` æ˜¯æ¸¸æˆæ‰‹æŸ„ä¸Šçš„ä¸¤ä¸ªä¸»è¦åŠ¨ä½œæŒ‰é’®**ã€‚å®ƒä»¬å¯¹åº”çš„åŠŸèƒ½å¦‚ä¸‹ï¼š

| æŒ‰é’® | ä½œç”¨                    | åœ¨é©¬é‡Œå¥¥ä¸­çš„å®é™…æ•ˆæœ                             |
| ---- | ----------------------- | ------------------------------------------------ |
| `A`  | **è·³è·ƒï¼ˆJumpï¼‰**        | æ§åˆ¶é©¬é‡Œå¥¥è·³èµ·æ¥                                 |
| `B`  | **åŠ é€Ÿï¼ˆRunï¼‰æˆ–å°„ç«çƒ** | è·‘æ­¥åŠ é€Ÿã€æ¸¸æ³³æ—¶åŠ é€Ÿã€å‘å°„ç«çƒï¼ˆå¦‚æœæœ‰ç«çƒèƒ½åŠ›ï¼‰ |

---

### Keyboard buttons controlling

`human_op.py` can provide you with mario playing.

> ğŸ® æ§åˆ¶è¯´æ˜ï¼šD=å³, A=å·¦, K=è·³, J=åŠ é€Ÿ, W=ä¸Š, S=ä¸‹ï¼Œæ”¯æŒç»„åˆé”®ï¼Œå¦‚ D+K

## Task Allocation

### work zone

- `Supervised` (Yonghai Yue)
- `Unsupervised` (Weiwei Lin)
- `Semi-supervised` (Yexin Liu Lu)

## Supervised Learning

input: game frame

output: action_id

### Data Collection

`collect_data.py`: Designed for collecting data.

ç©å®¶ä½¿ç”¨é¡»çŸ¥ï¼š

- ä½ å¯ä»¥è‡ªå·±**è®¾å®šå…³å¡**è¿›è¡Œæ”¶é›†è®­ç»ƒæ•°æ®ã€‚
- æ¯å½“**é€šå…³**æˆ–**æ­»äº¡**ä¼šç»“æŸæ•°æ®é‡‡é›†ã€‚

é‡‡é›†æ•°æ®è®¾ç½®ï¼š

- grayscale
- resize: `RESIZE_SHAPE = (84, 84)`
- frame skipping / subsampling: `FRAME_SKIP = 4 `
- recording after first moving: æ£€æŸ¥æ˜¯å¦å¼€å§‹é‡‡é›†: ç©å®¶å¼€å§‹ç§»åŠ¨ï¼Œå¼€å§‹é‡‡é›†æ•°æ®

æ•°æ®é›†æ–‡ä»¶å¤¹ç»“æ„ï¼š

```

supervised/
â””â”€â”€ mario_data/ â†ğŸ“‚ æ€»å…¥å£
â”œâ”€â”€ success/ â†âœ… ç©å®¶æˆåŠŸé€šå…³çš„æ¼”ç¤º
â”‚ â”œâ”€â”€ 1-1/ â†ğŸ§© å…³å¡ç¼–å·
â”‚ â”‚ â”œâ”€â”€ 10-12-37/ â†ğŸ“ å®éªŒæ—¶é—´æˆ³ç›®å½•
â”‚ â”‚ â”‚ â”œâ”€â”€ frames/
â”‚ â”‚ â”‚ â””â”€â”€ trajectory.json
â”‚ â”‚ â””â”€â”€ 11-09-05/
â”‚ â”‚ â”œâ”€â”€ frames/
â”‚ â”‚ â””â”€â”€ trajectory.json
â”‚ â”œâ”€â”€ 1-2/
â”‚ â”‚ â””â”€â”€ ...
â””â”€â”€ failure/ â†âŒ ç©å®¶æœªé€šå…³/å¤±è´¥çš„æ¼”ç¤º
â”œâ”€â”€ 1-1/
â”‚ â”œâ”€â”€ 10-15-00/
â”‚ â”‚ â”œâ”€â”€ frames/
â”‚ â”‚ â””â”€â”€ trajectory.json
â”‚ â””â”€â”€ ...
â”œâ”€â”€ 1-2/
â”‚ â””â”€â”€ ...

```

`MarioDataset.py` defines the `MarioDataset` class and the `train_val_split` function, both of which are used for data preparation prior to training.

### Training

`MarioTrainer.py` implements a subclass of `ClassifyTrainBase` with built-in functionalities for logging and plotting during training.

## Unsupervised

## Semi-supervised

```

```
